{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are tasked with cleaning and pre processing the real life data set of expenditure and profits of 50 Startups from 3 US states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('50_Startups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "#ptinting the read file to check if it is read properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove the duplicat values from the data set\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "#the number of different values of the categorical data\n",
    "df['State'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns =  df.columns[df.isnull().any()]\n",
    "num_emptycolumns = df[null_columns].isnull().sum()\n",
    "print(num_emptycolumns)\n",
    "\n",
    "#the output signifies the number of the null values in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the cleaning of data is done time for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,:-1].values\n",
    "y= df.iloc[:,-1].values\n",
    "\n",
    "#splitting the data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "label_encoder = LabelEncoder();\n",
    "x[:,3] = label_encoder.fit_transform(x[:,3])\n",
    "ck = ColumnTransformer([('one_hot_encoder', OneHotEncoder(), [3])],remainder='passthrough')\n",
    "#oneHotEncoder = OneHotEncoder(categorical_features=[3])\n",
    "x = np.array(ck.fit_transform(x), dtype=np.float64)\n",
    "#problem of dummy variable trap\n",
    "x= x[:,1:]  #to avoid the dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x\n",
    "#checking the data set after the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and test set\n",
    "from  sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    train_size=0.80, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after this we may start training the model based on various techniques, one example is to use Linear Regression done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n",
    "#prediction of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test\n",
    "#the actual values of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train score', regressor.score(X_train, y_train))\n",
    "print('Test score', regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the further code is only if we want to export the pre processed data to a csgv and is not necessary for training model\n",
    "we would append y to the end x and convert them to a data frame and then to a .csv file with a pre processesd data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z= np.concatenate((x,y.reshape(-1,1)),axis=1)\n",
    "\n",
    "n = pd.DataFrame(z)\n",
    "\n",
    "n.columns = ['D1','D2','R&D Spend (x0)','Administration(x1)','Marketing Spend(x2)','Profit(x3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.to_csv('50_Startups_preprocessed.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
